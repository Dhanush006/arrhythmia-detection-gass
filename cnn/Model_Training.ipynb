{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy3q3_0pGEEx"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import argparse \n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dtaATxJAOgn"
      },
      "source": [
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from sklearn.model_selection import KFold, StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHa2utBJAMqr"
      },
      "source": [
        "def reset_weights(m):\n",
        "  '''\n",
        "    Resetting model weights to avoid weight leakage.\n",
        "  '''\n",
        "  for layer in m.children():\n",
        "   if hasattr(layer, 'reset_parameters'):\n",
        "    print(f'Reset trainable parameters of layer = {layer}')\n",
        "    layer.reset_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0dkG_TvFoXY"
      },
      "source": [
        " def load_dataset(data_path):\n",
        "    transformation = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    full_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=data_path,\n",
        "        transform=transformation\n",
        "    )\n",
        "    \n",
        "    \n",
        "    train_size = int(0.7 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    \n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "    \n",
        "    return ConcatDataset([train_dataset, test_dataset])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSvCnBxOGUhG"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(in_channels=24, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        self.drop = nn.Dropout2d(p=0.2)\n",
        "        self.fc = nn.Linear(in_features=8 * 8 * 128, out_features=5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.pool(self.conv1(x)))  \n",
        "        x = F.relu(self.pool(self.conv2(x)))          \n",
        "        x = F.relu(self.pool(self.conv3(x)))  \n",
        "        x = F.relu(self.pool(self.conv4(x)))          \n",
        "        x = F.dropout(self.drop(x), training=self.training)\n",
        "        x = x.view(-1, 8 * 8 * 128)\n",
        "        x = self.fc(x)\n",
        "        return torch.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2fBZkKgGkPV"
      },
      "source": [
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "    device = \"cuda\"\n",
        "\n",
        "print('Training on', device)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_criteria(output, target)\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
        "    avg_loss = train_loss / (batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        samples_count = 0\n",
        "        for data, target in test_loader:\n",
        "            batch_count += 1\n",
        "            samples_count += len(target)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += loss_criteria(output, target).item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "\n",
        "    avg_loss = test_loss / batch_count\n",
        "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(avg_loss, correct, samples_count, 100. * correct / samples_count))\n",
        "    \n",
        "    return avg_loss, 100. * correct / samples_count\n",
        "\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "k_folds = 5\n",
        "num_epochs = 5\n",
        "results = {}\n",
        "\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "complete_dataset = load_dataset('/content/data/')\n",
        "\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "print('--------------------------------')\n",
        "\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(complete_dataset)):\n",
        "\n",
        "  model = Net(num_classes=5).to(device)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  print(f'FOLD {fold}')\n",
        "  print('--------------------------------')\n",
        "  \n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "                    complete_dataset, \n",
        "                    batch_size=25, sampler=train_subsampler)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "                    complete_dataset,\n",
        "                    batch_size=25, sampler=test_subsampler)\n",
        "\n",
        "  model.apply(reset_weights)\n",
        "  for epoch in range(0, num_epochs):\n",
        "\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "    train_loss = train(model, device, train_loader, optimizer, epoch+1)\n",
        "    epoch_nums.append(epoch)\n",
        "    training_loss.append(train_loss)\n",
        "\n",
        "  print('Training process has finished. Saving trained model.')\n",
        "  print('Starting testing')\n",
        "\n",
        "  test_loss, testaccuracy = test(model, device, test_loader)\n",
        "  validation_loss.append(test_loss)\n",
        "  results[fold] = testaccuracy\n",
        "\n",
        "\n",
        "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "print('--------------------------------')\n",
        "sum = 0.0\n",
        "for key, value in results.items():\n",
        "  print(f'Fold {key}: {value} %')\n",
        "  sum += value\n",
        "print(f'Average: {sum/len(results.items())} %')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7bhzckSIDxL"
      },
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "\n",
        "for fold in range(k_folds):\n",
        "  plt.plot(epoch_nums[k_folds * fold : k_folds * fold + num_epochs], training_loss[k_folds * fold : k_folds * fold + num_epochs])\n",
        "\n",
        "plt.plot(epoch_nums[:num_epochs], validation_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "classes = [\"L76\",\"N78\",\"Others\",\"R82\",\"Slash47\"]\n",
        "truelabels = []\n",
        "predictions = []\n",
        "model.eval()\n",
        "print(\"Getting predictions from test set...\")\n",
        "for data, target in test_loader:\n",
        "  for label in target.data.numpy():\n",
        "    truelabels.append(label)\n",
        "  for prediction in (model(data.cuda()).data).cpu().numpy().argmax(1):\n",
        "    predictions.append(prediction) \n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(truelabels, predictions)\n",
        "tick_marks = np.arange(len(classes))\n",
        "\n",
        "df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n",
        "plt.figure(figsize = (10,10))\n",
        "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
        "plt.xlabel(\"Predicted Shape\", fontsize = 20)\n",
        "plt.ylabel(\"True Shape\", fontsize = 20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB1DQnNLXHsR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}